# Job Search Pipeline Design
**Date:** Friday, February 27, 2026
**Time:** 3:21 PM PST
---

## User

I do not like the current applypivot strategy of finding jobs on the web. however, i think your standard web search tool may not be adequate enough as a replacement. we have so much information in this repository about me, we can make highly informed searches online. I am sure we can write a program to find recent, verifiable (not posted 30 days ago), real urls (there needs to be a 200 confirmation with the url, not a 400. the urls have to link to a job), job postings that are in some way matching my experiences. on top of that we should design it to have feature flags that you claude can use to modify depending on information in moons. ask me some questions, but what features do you see existing in this program? do you want to know why i dont like applypivot?

## Claude

```tool
[Tool: Read] /Users/jperr/Documents/Jobs/experience_inventory.md â€” full file
```

```tool
[Tool: Read] /Users/jperr/Documents/Jobs/graph.json â€” first 100 lines
```

```tool
[Tool: Glob] **/applypilot/** in /Users/jperr/Documents/Jobs
```

```tool
[Tool: Read] /Users/jperr/Documents/Jobs/applypilot/searches.yaml â€” full file
```

```tool
[Tool: Read] /Users/jperr/Documents/Jobs/applypilot/profile.json â€” full file
```

Yes â€” tell me why you don't like ApplyPilot. Understanding what's broken helps me design what's better.

Here's what I see as the feature set, informed by everything in your repo:

**Core search engine:**
- **Graph-driven query generation** â€” Instead of static search strings like `searches.yaml`, the program reads `graph.json` and builds queries from your actual skill/project/experience nodes. "mDNS service discovery engineer" is a real query that matches you. "AI engineer new grad" matches everyone.
- **Multi-source scraping** â€” Direct company career pages (Anthropic, OpenAI, Vercel, etc.), job board APIs, and aggregators. Not just Indeed/LinkedIn/Glassdoor.
- **URL verification** â€” Every result gets an HTTP request. 200 or discard. No dead links, no redirects to "this posting has expired."
- **Freshness enforcement** â€” Parse actual posting dates from page content, not just "posted 3 weeks ago" metadata that job boards lie about. Configurable max age (default: 7 days).

**Feature flags (Claude-modifiable):**
- `skill_emphasis` â€” Weight toward AI/ML, systems, backend, devtools, or research roles
- `experience_ceiling` â€” Max years required (currently filtering 7+, but configurable)
- `location_mode` â€” Remote only, Bay Area, hybrid, or expand radius
- `company_stage` â€” Startup, growth, FAANG, research lab
- `salary_floor` â€” Minimum compensation threshold
- `board_sources` â€” Which sources to query
- `archetype` â€” Maps directly to your `role_archetype` nodes in `graph.json`
- `exclude_companies` â€” Blacklist
- `freshness_days` â€” How recent

**Integrity checks:**
- **Deduplication** â€” SQLite store of seen URLs, never resurface the same job
- **Posting validation** â€” Confirm the page actually contains a job posting (not a blog post or company about page)
- **Company verification** â€” Basic check that the company exists and the careers page is real

**Integration with existing workflow:**
- Output compatible with `/evaluate` and `tracker.json`
- Each found job writes a structured record (title, company, URL, match score, posting date, skills matched)

Questions for you:

1. You said "feature flags that Claude can use to modify depending on information in moons" â€” did you mean "moods" or something else? I want to make sure I understand the modifier source.

2. What language do you want this in? Python seems natural given your stack, but TypeScript is also viable.

3. Should this replace the ApplyPilot database (`applypilot.db`) entirely, or write to its own store that `/evaluate` can also read from?

4. Do you have specific companies or career pages you want to always check, separate from the general search?

## User

I didnt like applypivot because it was slow, had no knowldge of my background, and often just scraped any job board in the hopes of quantity over quality. I want something that knows about ME and the entire pipeline has ME in mind while searching. This is why i think it is important we make the pipeline now. what is the algorithm or psuedocode for this. I tell you go find me some jobs today. you should go to moons and gain knowledge of my background, then perform multiple web searches ( maybe some teaching job searches, some ai job searches, some basic programming too, game dev, ...) dont try to force all these skills i have into one search, the filter will be too great to actually find good jobs. Then how can i trust the jobs you scrape are one you have read and actually scraped. should we convert the html pages to md first? how do we make sure when we do find a job we can say okay this is good and here is a url joey can see for himself. show me the pipeline!

## Claude

That criticism is clear and valid. ApplyPilot treats you as a generic input to a generic scraper. Here's the pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    JOEY SAYS "FIND JOBS"                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 1: KNOW JOEY                                         â”‚
â”‚                                                              â”‚
â”‚  Read experience_inventory.md                                â”‚
â”‚  Read graph.json                                             â”‚
â”‚  Read feature_flags.yaml (Claude-modifiable config)          â”‚
â”‚                                                              â”‚
â”‚  Extract SKILL CLUSTERS (not one mega-query):                â”‚
â”‚    cluster_ai      = [llm-engineering, prompt-engineering,   â”‚
â”‚                       structured outputs, multi-model]       â”‚
â”‚    cluster_systems  = [mDNS, networking, protocol design,    â”‚
â”‚                       distributed systems, cross-compilation]â”‚
â”‚    cluster_backend  = [Python, FastAPI, asyncio, PostgreSQL,  â”‚
â”‚                       API design, Docker]                    â”‚
â”‚    cluster_teaching = [TA, instructor, curriculum, tutoring]  â”‚
â”‚    cluster_gamedev  = [game AI, MCTS, Unity, procedural gen] â”‚
â”‚    cluster_devtools = [TypeScript, npm, Vercel AI SDK,        â”‚
â”‚                       package publishing, open source]       â”‚
â”‚    cluster_research = [MS thesis, computational media,        â”‚
â”‚                       evaluation frameworks]                 â”‚
â”‚                                                              â”‚
â”‚  Each cluster gets its OWN search pass.                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 2: GENERATE QUERIES                                   â”‚
â”‚                                                              â”‚
â”‚  For each cluster, generate 2-4 targeted search queries:     â”‚
â”‚                                                              â”‚
â”‚  cluster_ai â†’                                                â”‚
â”‚    "AI engineer entry level remote 2026"                     â”‚
â”‚    "LLM infrastructure engineer new grad"                    â”‚
â”‚    "ML engineer Python OpenAI API"                           â”‚
â”‚                                                              â”‚
â”‚  cluster_teaching â†’                                          â”‚
â”‚    "computer science instructor Bay Area"                    â”‚
â”‚    "coding bootcamp teacher Python"                          â”‚
â”‚    "CS teaching position university 2026"                    â”‚
â”‚                                                              â”‚
â”‚  cluster_gamedev â†’                                           â”‚
â”‚    "game AI engineer"                                        â”‚
â”‚    "procedural generation developer"                         â”‚
â”‚                                                              â”‚
â”‚  ... etc for each cluster                                    â”‚
â”‚                                                              â”‚
â”‚  Apply feature flags:                                        â”‚
â”‚    if flags.location_mode == "remote_only":                  â”‚
â”‚      append "remote" to all queries                          â”‚
â”‚    if flags.freshness_days == 3:                             â”‚
â”‚      append date filters where supported                     â”‚
â”‚    if flags.exclude_companies:                               â”‚
â”‚      note for later filtering                                â”‚
â”‚                                                              â”‚
â”‚  Total: ~15-25 distinct searches per run                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 3: SEARCH (parallel)                                  â”‚
â”‚                                                              â”‚
â”‚  For each query:                                             â”‚
â”‚    results = web_search(query)                               â”‚
â”‚    collect raw URLs + titles + snippets                      â”‚
â”‚                                                              â”‚
â”‚  Aggregate all results into candidate_urls[]                 â”‚
â”‚  Deduplicate against seen_jobs.db (SQLite)                   â”‚
â”‚  Remove URLs matching exclude patterns                       â”‚
â”‚    (blog posts, "about us" pages, news articles)             â”‚
â”‚                                                              â”‚
â”‚  Output: ~50-150 candidate URLs after dedup                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 4: FETCH + VERIFY (parallel, batched)                 â”‚
â”‚                                                              â”‚
â”‚  For each candidate URL:                                     â”‚
â”‚    response = HTTP GET(url)                                  â”‚
â”‚                                                              â”‚
â”‚    âŒ DISCARD if:                                             â”‚
â”‚      - status != 200                                         â”‚
â”‚      - redirects to generic careers page (not a specific job)â”‚
â”‚      - domain is a known aggregator re-posting stale jobs    â”‚
â”‚                                                              â”‚
â”‚    âœ… KEEP if:                                                â”‚
â”‚      - status == 200                                         â”‚
â”‚      - page contains job-posting signals                     â”‚
â”‚        (apply button, requirements section, salary info)     â”‚
â”‚                                                              â”‚
â”‚    Convert HTML â†’ Markdown (strip nav, footer, ads)          â”‚
â”‚    Store as:                                                 â”‚
â”‚      {                                                       â”‚
â”‚        url: "https://company.com/jobs/12345",                â”‚
â”‚        status: 200,                                          â”‚
â”‚        markdown: "# Software Engineer\n\n## About...",      â”‚
â”‚        fetched_at: "2026-02-27T15:30:00Z"                   â”‚
â”‚      }                                                       â”‚
â”‚                                                              â”‚
â”‚  Output: verified_jobs[] with markdown content               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 5: READ + SCORE (this is where Claude proves it)      â”‚
â”‚                                                              â”‚
â”‚  For each verified job, Claude READS the markdown and:       â”‚
â”‚                                                              â”‚
â”‚  1. Extracts structured fields:                              â”‚
â”‚     - title, company, location, remote?, salary_range        â”‚
â”‚     - required_skills[], preferred_skills[]                  â”‚
â”‚     - years_required, education_required                     â”‚
â”‚     - posting_date (parsed from page content)                â”‚
â”‚                                                              â”‚
â”‚  2. Scores against graph.json:                               â”‚
â”‚     match_score = 0                                          â”‚
â”‚     for skill in required_skills:                            â”‚
â”‚       if skill in joey_skills:                               â”‚
â”‚         match_score += weight(skill.proficiency)             â”‚
â”‚     for skill in preferred_skills:                           â”‚
â”‚       if skill in joey_skills:                               â”‚
â”‚         match_score += weight(skill.proficiency) * 0.5       â”‚
â”‚     penalty if years_required > 3                            â”‚
â”‚     penalty if location doesn't match flags                  â”‚
â”‚     bonus if company_stage matches flags                     â”‚
â”‚                                                              â”‚
â”‚  3. Writes a PROOF record:                                   â”‚
â”‚     {                                                        â”‚
â”‚       url: "https://...",        â† Joey clicks this          â”‚
â”‚       title: "AI Engineer",                                  â”‚
â”‚       company: "Acme Corp",                                  â”‚
â”‚       match_score: 0.82,                                     â”‚
â”‚       matched_skills: ["Python", "LLM", "FastAPI"],          â”‚
â”‚       missing_skills: ["Kubernetes"],                        â”‚
â”‚       posting_date: "2026-02-25",                            â”‚
â”‚       summary: "3-sentence summary of the role",             â”‚
â”‚       why_match: "Saturn demonstrates their core reqs...",   â”‚
â”‚       source_cluster: "cluster_ai"                           â”‚
â”‚     }                                                        â”‚
â”‚                                                              â”‚
â”‚  âŒ DISCARD if:                                               â”‚
â”‚    - match_score < threshold (default 0.4)                   â”‚
â”‚    - posting_date older than flags.freshness_days             â”‚
â”‚    - years_required > flags.experience_ceiling               â”‚
â”‚                                                              â”‚
â”‚  Output: scored_jobs[] â€” only jobs Claude actually read      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PHASE 6: PRESENT TO JOEY                                    â”‚
â”‚                                                              â”‚
â”‚  Sort scored_jobs by match_score DESC                         â”‚
â”‚  Group by source_cluster                                     â”‚
â”‚                                                              â”‚
â”‚  Output format (per job):                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ â¬› AI Engineer â€” Acme Corp (0.82)                  â”‚      â”‚
â”‚  â”‚ ğŸ”— https://acme.com/jobs/12345  â† VERIFIED LIVE   â”‚      â”‚
â”‚  â”‚ ğŸ“ Remote | Posted: Feb 25, 2026                  â”‚      â”‚
â”‚  â”‚ âœ… Matched: Python, LLM, FastAPI, API design       â”‚      â”‚
â”‚  â”‚ â“ Missing: Kubernetes (nice-to-have)              â”‚      â”‚
â”‚  â”‚ ğŸ’¬ "Saturn directly demonstrates their need for    â”‚      â”‚
â”‚  â”‚    multi-model routing and service discovery..."   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚  Write results to jobs_found.json                            â”‚
â”‚  Insert new URLs into seen_jobs.db                           â”‚
â”‚  Log run metadata (queries used, counts, timestamp)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**On your HTMLâ†’MD question:** Yes. Converting to markdown is the right move. It strips the noise (nav bars, footers, tracking scripts, cookie banners) and gives Claude clean text to actually parse. Raw HTML is full of garbage that makes extraction unreliable. The markdown becomes the proof artifact â€” "here is what I read, here is what I extracted from it."

**On trust:** The proof record is the answer. Every job in the output has: the URL you can click, the skills Claude matched against YOUR graph, and a `why_match` explanation referencing YOUR projects. If Claude can't explain why it matched, it doesn't show up.

**On "moons"** â€” I'm still unclear on this word. Did you mean your memory files / experience docs? Or something else?

One design question: should the program be a standalone Python script you run (`python find_jobs.py`), or should it be a Claude skill (`/find-jobs`) that I execute using my own tools (WebSearch, WebFetch) within conversation? The skill approach means I'm doing the searching and reading live, with no external dependencies. The script approach is more automated but needs API keys for search.

## User

create a plan to implement this and save it as PLAN.md. when you are done just say I made PLAN.md

## Claude

```tool
[Tool: Write] /Users/jperr/Documents/Jobs/PLAN.md â€” Full implementation plan for /find-jobs skill covering 6 phases, search_config.yaml structure, seen_jobs.json, skill prompt, and integration with existing pipeline
```

I made PLAN.md

## User

does it also include removing any mention of applypivot in our system?

## Claude

No. The plan covers building the new `/find-jobs` skill but doesn't include removing ApplyPilot references. That should be a step. Specifically:

- `CLAUDE.md` â€” the pipeline section still references `applypilot run discover enrich` and `APPLYPILOT_DIR`
- `applypilot/` directory â€” `searches.yaml`, `profile.json`, `applypilot.db`, `.env`
- `~/.zshrc` â€” the `APPLYPILOT_DIR` env var
- Any skills that reference `applypilot.db` (the `/evaluate` skill reads from it)

Want me to add a cleanup step to `PLAN.md` that removes all of this?

## User

yes

## Claude

```tool
[Tool: Read] /Users/jperr/Documents/Jobs/PLAN.md â€” full file
```

```tool
[Tool: Edit] /Users/jperr/Documents/Jobs/PLAN.md â€” Added Step 5 (Remove ApplyPilot) and Step 6 (Update CLAUDE.md Key Files), covering deletion of applypilot/ directory, CLAUDE.md cleanup, ~/.zshrc cleanup, /evaluate skill rewiring, and new pipeline docs
```

Done. Added Step 5 (Remove ApplyPilot) and Step 6 (Update CLAUDE.md Key Files) to `PLAN.md`. The cleanup covers:

- Deleting the entire `applypilot/` directory
- Stripping all ApplyPilot references from `CLAUDE.md`
- Removing `APPLYPILOT_DIR` from `~/.zshrc`
- Rewiring or retiring the `/evaluate` skill so it no longer reads `applypilot.db`
- Replacing the pipeline docs with the new `/find-jobs` flow

## User

I am done documenting.
